<html>
<title> - ARTIFICIAL INTELLIGENCE AND HUMAN RIGHTS</title>
<body><pre>
[Senate Hearing 118-40]
[From the U.S. Government Publishing Office]





                                                         S. Hrg. 118-40
 
                        ARTIFICIAL INTELLIGENCE
                            AND HUMAN RIGHTS

=======================================================================

                                HEARING

                               before the

                SUBCOMMITTEE ON HUMAN RIGHTS AND THE LAW

                                 of the

                       COMMITTEE ON THE JUDICIARY
                          UNITED STATES SENATE

                    ONE HUNDRED EIGHTEENTH CONGRESS

                             FIRST SESSION

                               __________

                             JUNE 13, 2023

                               __________

                          Serial No. J-118-21

                               __________

         Printed for the use of the Committee on the Judiciary
         
         
        [GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]
        
        
        
        
                                  ______

             U.S. GOVERNMENT PUBLISHING OFFICE 
 00-000          WASHINGTON : 2024
       
        
        

                       COMMITTEE ON THE JUDICIARY

                   RICHARD J. DURBIN, Illinois, Chair
DIANNE FEINSTEIN, California         LINDSEY O. GRAHAM, South Carolina, 
SHELDON WHITEHOUSE, Rhode Island             Ranking Member
AMY KLOBUCHAR, Minnesota             CHARLES E. GRASSLEY, Iowa
CHRISTOPHER A. COONS, Delaware       JOHN CORNYN, Texas
RICHARD BLUMENTHAL, Connecticut      MICHAEL S. LEE, Utah
MAZIE K. HIRONO, Hawaii              TED CRUZ, Texas
CORY A. BOOKER, New Jersey           JOSH HAWLEY, Missouri
ALEX PADILLA, California             TOM COTTON, Arkansas
JON OSSOFF, Georgia                  JOHN KENNEDY, Louisiana
PETER WELCH, Vermont                 THOM TILLIS, North Carolina
                                     MARSHA BLACKBURN, Tennessee
             Joseph Zogby, Chief Counsel and Staff Director
      Katherine Nikas, Republican Chief Counsel and Staff Director

                Subcommittee on Human Rights and the Law

                       JON OSSOFF, Georgia, Chair
DIANNE FEINSTEIN, California         MARSHA BLACKBURN, Tennessee, 
RICHARD BLUMENTHAL, Connecticut          Ranking Member
PETER WELCH, Vermont                 JOHN KENNEDY, Louisiana
                                     JOSH HAWLEY, Missouri
               Sara Schaumburg, Democratic Chief Counsel
                 Kaitlyn Lane, Republican Chief Counsel
                            C O N T E N T S

                              ----------                              

                        JUNE 13, 2023, 2:36 P.M.

                    STATEMENTS OF COMMITTEE MEMBERS

                                                                   Page

Ossoff, Hon. Jon, a U.S. Senator from the State of Georgia.......     1
Blackburn, Hon. Marsha, a U.S. Senator from the State of 
  Tennessee......................................................     2

                               WITNESSES

Witness List.....................................................    31
Cain, Geoffrey, senior fellow, Foundation for American 
  Innovation, Chicago, Illinois..................................    11
    prepared statement...........................................    32
DeStefano, Jennifer, Victim of AI Deepfake Kidnapping and 
  Extortion Scam, Scottsdale, Arizona............................     4
    prepared statement...........................................    40
Givens, Alexandra Reeve, president and chief executive officer, 
  Center for Democracy and Technology, Washington, DC............     9
    prepared statement...........................................    47
Madry, Aleksander, Cadence Design Systems, Professor of 
  Computing, Massachusetts Institute of Technology, Cambridge, 
  Massachusetts..................................................     7
    prepared statement...........................................    58



                        ARTIFICIAL INTELLIGENCE



                            AND HUMAN RIGHTS

                              ----------                              


                         TUESDAY, JUNE 13, 2023

                      United States Senate,
          Subcommittee on Human Rights and the Law,
                                Committee on the Judiciary,
                                                    Washington, DC.
    The Subcommittee met, pursuant to notice at 2:36 p.m., in 
Room 226, Dirksen Senate Office Building, Hon. Jon Ossoff, 
Chair of the Subcommittee, presiding.
    Present: Senators Ossoff [presiding], Blumenthal, Welch, 
Blackburn, Kennedy, and Hawley.
    Also present: Chair Durbin and Senator Padilla.

             OPENING STATEMENT OF HON. JON OSSOFF,
            A U.S. SENATOR FROM THE STATE OF GEORGIA

    Chair Ossoff. The Subcommittee on Human Rights and the Law 
will come to order. Welcome all to today's hearing. It is great 
to see a packed house. It demonstrates the intensity of 
interest in this subject. I want to thank you, Ranking Member 
Blackburn, for working so hard and so closely with me to 
develop this important bipartisan hearing. And I want to thank 
each of our witnesses for your participation today.
    Throughout history, transformative technologies have 
emerged with the potential to disrupt societies, economies, and 
politics profoundly and sometimes very quickly. Machine 
learning and artificial intelligence may be such a technology. 
AI capabilities are growing rapidly and in ways even its 
creators cannot predict. And already it's changing our lives. 
American families are now threatened by AI-enabled scams made 
far more sophisticated through this technology than traditional 
spam email or sham telemarketing calls.
    Today we will hear from Jennifer DeStefano, who was 
targeted by a scam using a deepfake of her 15-year-old 
daughter's voice to fake her kidnapping and extort a ransom 
payment.
    AI also has profound implications for civil rights, for the 
criminal justice system, for our democratic and constitutional 
processes, and for our privacy. Its potential impact on the 
future of work could include fundamental shifts in education, 
in recruitment, candidate screening and hiring, and perhaps 
even more significantly, rapid disruption of labor markets as 
certain professions are automated.
    This technology has profound implications for the future of 
warfare, as kill chains are automated and predictive technology 
influences and mediates competition between nation states. As 
AI technology develops, great powers competing in an AI arms 
race engaged in strategic competition, where AI is influencing 
the decisions made by leaders and militaries, face a different 
and new risk of escalation and miscalculation.
    And some influential technologists and engineers, including 
prominent figures and prominent leaders of the industry, warn 
of existential risks ranging from catastrophic political 
destabilization to the development and deployment of weapons of 
mass destruction, to catastrophic cybersecurity threats, and to 
unforeseeable and unknown forms of risk that may emerge 
alongside more and more powerful forms of artificial 
intelligence.
    Our study of these technologies and associated risks should 
not blind us, of course, to this technology's extraordinary 
potential. For example, cancer diagnoses, the development of 
new life-saving drugs and therapies, productivity growth, and 
the new forms of technological innovation that AI itself could 
help us to unlock.
    But at a moment like this, it is imperative that Congress 
understand the full range of risks and potentials to ensure 
this technology can be developed, deployed, used, and regulated 
consistent with our core values, consistent with our national 
interest, consistent with civil and human rights. So I look 
forward to a productive conversation with this talented and 
extraordinary panel this afternoon. And with that, I turn to 
the Ranking Member of the Subcommittee, my colleague from 
Tennessee, Senator Blackburn.

          OPENING STATEMENT OF HON. MARSHA BLACKBURN,
           A U.S. SENATOR FROM THE STATE OF TENNESSEE

    Senator Blackburn. Thank you, Mr. Chairman. I am delighted 
that we're getting the Subcommittee off the starting blocks 
today. So I thank you and your team for the good work on those 
efforts and focusing on something where we do share an 
interest, which is artificial intelligence and technology, and 
the uses that you say, for good or for bad.
    I do want to touch on China, and I'm so pleased that we're 
looking at this from the human rights angle. I've watched what 
has happened in China and how they are using AI to grow the 
surveillance state. And they're very aggressive in this. And we 
know that they have used it--a good example is the way they 
have exploited vulnerabilities in Apple's iPhone in the 
iMessage system to surveil and track the Uyghur Muslims in 
Xinjiang Province.
    And the CCP uses facial recognition as a part of their 
tracking, and a part of their data, and the logging of 
information that they do as they're following people. And we 
want to dive into that a little bit. We know that China is 
pushing to win the race on AI. They've been very upfront about 
this, and they are looking to win the race on other 
technologies.
    Quantum computing, 5G, 6G, anything that they see as 
groundbreaking that helps them to control environments, 
situations, and people. I think the data from McKinsey & 
Company should be something that we all look at and take to 
heart. They predict that by 2030, China's growth in AI could 
account for up to $600 billion in economic value. And this is 
exactly what they want.
    And in 2017, the National AI Development Plan that they 
brought forward, China declared its goal of becoming the world 
leader in AI by 2030. And they're pursuing this. They're the 
most aggressive filer of patents for AI technologies. They are 
constantly challenging our innovators through the PTAB process.
    So we should be watching their goals. And this should 
concern each and every one of us who cares about preserving the 
freedoms and the democratic values that we hold here in 
America. As we work to deploy AI technologies here, we need to 
make a conscious effort to consider the potential impact that 
those technologies could have on human rights and on how we 
approach issues such as data collection, data retention, 
surveillance, and, of course, deepfakes.
    This is not to say that we should halt AI development in 
its tracks or look at approaches that would regulate it out of 
existence. To the contrary, doing that would practically 
guarantee that China becomes the world's leader in AI, giving 
it the opening that President Xi wants to impose the CCP's 
authoritarian values around the world. But we do need to think 
carefully about how we deploy AI technologies in the absence of 
a national privacy law, which we still do not have, a Federal 
online consumer privacy protection.
    We also need to be careful about how we identify and how we 
stop unauthorized utilizations of AI, whether to surveil or to 
scam unsuspecting people. So to our witnesses, thank you for 
being with us today. Mr. Chairman, I appreciate the hearing. 
Look forward to moving to questions.
    Chair Ossoff. Thank you, Senator Blackburn. I will now 
introduce our witnesses, and thank you, again, for joining us 
today.
    Ms. Jennifer DeStefano, mother from Arizona, was the victim 
of a horrifying scam using an AI-generated deepfake of her 
daughter's voice to fake her kidnapping and demand a ransom. 
Ms. DeStefano, I think every parent in America who read your 
story was chilled to the bone by what you experienced. We'll 
hear from you, Ms. DeStefano, about your experience to help 
shed light on how AI is being used to supercharge extortion-
based scams and threaten the safety of American families.
    Dr. Aleksander Madry is a nationally recognized expert on 
AI and machine learning whose research focuses on how to ensure 
AI tools are reliable and well-enough understood to be safely 
and responsibly deployed in the real world. Thank you, Dr. 
Madry.
    Ms. Alexandra Reeve Givens is the CEO of the Center for 
Democracy and Technology, which works to ensure emerging 
technologies protect democratic values and advance human 
rights.
    And Mr. Geoffrey Cain is a Senior Fellow at the Foundation 
for American Innovation, a technologist and author who studies 
how repressive governments deploy novel technologies and how 
democracies can respond and defend human rights.
    Thank you all so much for joining. Before your opening 
statements we will swear in our witnesses. If you would all 
please rise and raise your right hands?
    [Witnesses are sworn in.]
    Chair Ossoff. Let the record reflect the witnesses have 
responded in the affirmative. You may be seated. And Ms. 
DeStefano, we'll begin please with your opening statement. 
You'll see some lights indicating time, but we want to make 
sure you have time to tell your whole story. So don't worry too 
much about the clock. We're eager to hear from you. And you may 
begin.

      STATEMENT OF JENNIFER DeSTEFANO, VICTIM OF AI DEEP-
    FAKE KIDNAPPING AND EXTORTION SCAM, SCOTTSDALE, ARIZONA

    Ms. DeStefano. Thank you so much, Senator. I appreciate 
that. Good afternoon, Senators. It is my great honor to speak 
with you today and share my experience on how artificial 
intelligence is being weaponized to not only invoke fear and 
terror in the American public but in the global community at 
large as it capitalizes on and redefines what we have known as 
familiar.
    I would like to take this moment to thank Senator Ossoff 
for inviting me to be here today, and I'd also like to thank 
Senator Blackburn for your concern on this ever-evolving topic 
and community threat. AI is revolutionizing and unraveling the 
very foundation of our social fabric by creating doubt and fear 
in what was once never questioned, the sound of a loved one's 
voice.
    What is familiar? How many times have you received a phone 
call from your child and asked them to verify who is calling? 
How many times has a loved one reached out to you in despair 
and you stopped them to validate their identity? Did you hang 
up on them? Did you require to call them back to make sure you 
are speaking to the correct person? The answer is, more than 
likely, never. The sound of a loved one's voice is often never 
authenticated. It has a unique identity, as unique as a 
fingerprint. This familiar identity is innate and is designed 
by God. It is what binds a mother to their child and a newborn 
infant to their mother.
    January 20th was a typical Friday afternoon for our family, 
kicking off a weekend of races and rehearsals. We often divide 
our family across the State. It's divide and conquer. My 
husband was with our older daughter, Brie, training for a ski 
race, and I was with my younger daughter, Aubrey, picking her 
up from a rehearsal at dance. Brie had not raced in years and 
promised me that she would take it easy.
    At about 4:53 p.m., I received a call from an unknown 
number upon exiting my car. At the final ring I chose to answer 
it, as unknown calls we're very familiar with--can often be a 
hospital or a doctor. It was Briana sobbing and crying, saying, 
``Mom?'' At first, I thought nothing of it and casually asked 
her, ``What happened?'' I had the phone on speaker, walking 
through the parking lot to meet her sister.
    Briana continued with, ``Mom, I messed up,'' crying and 
sobbing continually. Not thinking twice, I asked her again, 
``Okay, what happened?'' Suddenly, a man's voice barked at her, 
``Lay down. Put your head back.'' At that moment, I started to 
panic. My concern escalated as I demanded to know what was 
going on. But nothing could have prepared me for her response 
that she gave me next.
    ``Mom, these bad men have me. Help me, help me, help me.'' 
She begged and pleaded as the phone was taken from her. A 
threatening and vulgar man took the call over. ``Listen here, I 
have your daughter. You call anybody, you call the police, I'm 
going to pop her stomach so full of drugs, I'm going to have my 
way with her, I'm going to drop her in Mexico, and you'll never 
see your daughter again.''
    As I had my hand shaking on the door handle of the dance 
studio, I ran inside and started screaming for help. The next 
few minutes were every parent's worst nightmare. I was 
fortunate to have a couple of moms there who knew me well, and 
they instantly went to action.
    One mom ran outside and called 911. My younger daughter 
Aubrey was standing there listening to all the vulgar threats 
this man was making that he was going to do to her sister. I 
needed her help and asked her to start calling her dad, call 
her brothers, call anybody we have to find her sister. She 
stood there paralyzed in fear.
    The second mom ran to Aubrey's aid and started making calls 
to her dad. The kidnapper demanded a million dollars. That was 
not possible. So then he decided on $50,000 in cash. That was 
when the first mom came back in and told me that 911 is very 
familiar with an AI scam where they can use someone's voice. 
But I didn't process that. It wasn't just her voice. It was her 
cries. It was her sobs. It was just not her voice. She said 
okay and left.
    I continued with the negotiations for the ransom. I asked 
them for wiring instructions, routing numbers, but they 
refused. Instead, they required me to get in a van with a bag 
over my head with $50,000 in cash to be transported to my 
daughter. If I didn't have all the money then we were both 
going to be dead. I was shocked.
    At that point in time the second mom came back to me, and 
she had located my husband who had found Brie resting safely in 
bed. She came to me and told me that Briana was safe, but I did 
not believe her because I had just spoken to my daughter, and I 
was very sure of her voice, and I was very sure of her cries. 
So I demanded to talk to my daughter.
    Briana got on the phone, and she had no idea what was going 
on, and she kept reassuring me that she was safe. I asked her 
so many times, ``Are you sure? Are you sure you're safe? Are 
you sure you're with dad? I spoke to you. How can you be in 
both places at once?'' I asked her over and over again. My mind 
was whirling.
    When I finally had the reassurance I needed, I knew she was 
safe, and I was furious. I lashed at the men for the horrible 
attempt to scam and extort money. They continued to threaten to 
kill Brie. I made a promise that I was going to stop them and 
they were never going to hurt my daughter nor anybody else 
again.
    At that point, I hung up and collapsed to the floor in 
tears of relief. I called the police to pursue the matter, and 
unfortunately, I was met with, ``It was a prank call,'' that it 
happens often, and that there's nothing that can be done, and 
that I probably am not in harm's way but it's not a guarantee. 
They offered to have a police officer contact me, again from an 
unknown number, as authorities are calling from blocked 
numbers. But that's all they could offer. That certainly did 
not make me feel better.
    The bottom line was no actual crime had been committed, so 
no physical kidnapping had taken place and no money had 
transferred, period. The end. But that wasn't the end. It 
couldn't be the end. If it was the end then how would this 
nightmare ever stop? I stayed up all night paralyzed in fear. 
``Do they know where I am? Do they know where my daughter is? 
How did they get her voice? How did they get her crying, her 
sobs that are unique to her?'' She is not a very public person. 
I was wondering, ``Are we being cyber-stalked? Targeted?''
    So many questions that were left unanswered. So I turned to 
the community, and the responses were overwhelming. Friends and 
neighbors came out of the woodwork with their stories. 
Kidnapping, phone calls coming from their children's phones, 
bags of money being driven halfway to Mexico, even voices of 
young children nowhere to be found on social media who do not 
have phones. The stories kept pouring in.
    My own mother even received a phone call with my brother's 
voice, claiming to be in an accident needing money for a 
hospital bill. The common response that victims received from 
authorities when reported was that nothing could be done. In 
fact, one mother I know personally shared with me how she was 
even mocked by her son's school and a security officer. The 
caller even used her son's unique nickname to self-identify. 
Fortunately, he was safe in class, and she was told this 
happens all the time as her fear was dismissed.
    Money scams have been around for thousands of years. This 
is entirely different. This is terrorizing, lasting trauma. 
Even months later sharing the story makes me shake to my core. 
Aubrey was approached by a boy to hang out sometime, and she 
concluded it was because he wants to kidnap her. That's not a 
normal 13-year-old thought. It was my daughter's voice. It was 
her cries. It was her sobs. It was the way she spoke.
    I will never be able to shake that voice and the desperate 
cries for help out of my mind. It's every parent's worst 
nightmare to hear your child pleading with fear and pain, 
knowing that they are being harmed and you're helpless. The 
longer this form of terror remains unpunishable, the farther 
and more egregious it will become. There is no limit to the 
depth of evil AI can enable.
    The thought crossed my mind before I hung up on the 
kidnappers to follow through with the physical abduction of me. 
Was that what it would take to bring this to an end? Was that 
what it would take in order to have a punishable criminal 
offense?
    As our world moves at a lightning-fast pace, the human 
element of familiarity that lays foundation to our social 
fabric of what is known and what is truth is being 
revolutionized with AI, some for good and some for evil. No 
longer can we trust, ``Seeing is believing,'' or, ``I heard it 
with my own ears,'' or even the sound of your own child's 
voice.
    The concept redefines and rewrites what the very meaning of 
familiarity means. I ask you, when your mother calls are you 
going to hang up on her and call her back to make sure it's 
her? When your child calls in need of help will you end the 
call and say, ``I don't believe it's really you''? Is this our 
new normal? Is this the future we are creating by enabling the 
abuses of artificial intelligence without consequence and 
without regulation?
    I want to thank you for your time and attention today. 
Congress has a large and looming task ahead. How do we move 
forward as a community with this haunting reality that is 
plaguing us? If left uncontrolled, unregulated, and we are left 
unprotected without consequence, it will rewrite our 
understanding and perception of what is and what is not truth. 
It will erode our sense of familiar as it corrodes our 
confidence in what is real and what is not. This is a non-
partisan matter, and I've seen the hands reach across the aisle 
in unified concern.
    That gives me great hope. How to contain the ever-evolving 
artificial intelligence and its unknowns is not an easy task. 
My sincere thanks and humble appreciation for your time and 
attention today. I thank all of you, especially Senator Ossoff 
and the Senate at large, for tirelessly taking action to keep 
our community and our world safe from the hands of evil. I am 
one person, one story, but I'm not the only one. And I 
certainly will not be the last unless action is taken. I wish 
you Godspeed.
    [The prepared statement of Ms. DeStefano appears as a 
submission for the record.]
    Chair Ossoff. Thank you, Ms. DeStefano, for sharing your 
powerful and disturbing story. And we will in more detail 
investigate all of the issues you've raised. I appreciate it. 
Dr. Madry, it's now your turn for an opening statement. Thank 
you.

    STATEMENT OF ALEKSANDER MADRY, CADENCE DESIGN SYSTEMS, 
PROFESSOR OF COMPUTING, MASSACHUSETTS INSTITUTE OF TECHNOLOGY, 
                    CAMBRIDGE, MASSACHUSETTS

    Dr. Madry. Thank you. Chairman Ossoff, and Ranking Member 
Blackburn, and Members of the Committee, thank you for inviting 
me to testify today. I must say it's hard to follow this 
testimony, in part because honestly it makes way better some of 
the points I wanted to make, but let me try nonetheless.
    So I want to focus my testimony on a single issue that I 
find particularly salient, time sensitive, and really 
unsettling: how AI could undermine our whole information 
ecosystem, and with that erode how our society functions and 
carries out democratic decisionmaking.
    The newest wave of generative AI is poised to fundamentally 
transform our collective sense making. And this is due to two 
reasons. First, AI enables the creation of content that is not 
only extremely realistic but also persuasive, even though it 
may be false. Second, with AI, the creation of such content is 
cheap and broadly accessible, making it frighteningly easy to 
deploy at scale.
    As a result, a whole spectrum of risks is emerging. 
Firstly, traditional spam, scam, and phishing become even 
easier to conduct. Also, AI can now convincingly impersonate a 
human online or over the phone. That was a frightening but very 
real experience that we just have heard about. So how will our 
digital platforms cope with swarms of AI-driven bots that can 
breeze through existing bot detection and moderation 
algorithms?
    And the worst thing is that such boosting of traditional 
deception is just the beginning, not the end. So AI is now able 
to create content that is both convincing and personalized. 
This means that phishing no longer needs to involve generic 
emails sent out to thousands of recipients. Instead, both the 
message and the ensuing conversation can be fully automated and 
customized to you.
    AI is also bound to transform how we think about any 
information campaign, be it ideological, political, or 
commercial. Such campaigns will no longer need to rely on a 
promoted message to go viral. Instead such campaigns they can 
be filtered to generative AI that reaches an internet audience 
individually and in a highly personalized manner.
    So the hook to get you will not be some post that came 
across your social media. Rather, it might be a Facebook friend 
who is actually an AI-driven agent impersonating a human, a 
friend that only subtly mixes in political commentary or 
product endorsement into your engaging conversations.
    Similarly, campaigning for a cause might no longer require 
corralling a critical mass of people to do the outreach, be it 
via direct calling or letter writing. Instead, a single actor 
could fill such a campaign by themselves, using generative AI-
driven bots in place of people. Such a campaign would be 
equally effective, but it would need neither the buy-in from 
the broader population nor comparable resources. And as far as 
I know, this would all probably be legal, too.
    Also, AI doesn't just produce content that is personalized. 
It can also make this content be personable. This could be used 
to make interacting with AI not only persuasive but also 
alluring to the point of being addictive. What if these 
capabilities supercharge the attention economy, or rather 
distraction economy, that we are having right now? How will we 
feel about having our children be exposed to that?
    Finally, AI is ushering us into the era where any record--a 
contract, a deposition, a video--could be plausibly faked. How 
does this affect our collective discourse as well as the legal 
and governance system? All of these concerns may paint a rather 
bleak picture, but there's actually much that can be done.
    On the technical front, we need tools that help humans 
judge the extent to which a given content was generated by a 
human. However, these tools are still developing, and they will 
not be a panacea. Rather, they can provide the necessary 
friction that makes it harder to abuse AI capabilities. They 
will also not work to the full extent, and in some cases at 
all, without complementary policy developments.
    In particular, the efficacy of these technical approaches 
will hinge on how broadly adopted they are. Policy can 
accelerate this process. Policies could also require any 
consumer-facing, AI-generating content to be labeled as such. 
And policy could also mandate providers of AI services to 
implement adequate identification and reporting mechanisms.
    Finally, we do need to work on AI literacy. I think that no 
matter what happens in the end, the public needs to understand 
how to judiciously interact with AI systems and to be on the 
lookout for when they are actually interacting with AI in the 
first place. We really do not want to learn this the hard way, 
the way we have seen it over here.
    So to conclude, I am really excited about the positive 
impacts that AI can have, but we need to be mindful of the very 
real risks, and we need to get started now. Thank you, and I'm 
looking forward to the questions.
    [The prepared statement of Dr. Madry appears as a 
submission for the record.]
    Chair Ossoff. Thank you, Dr. Madry. And I would note that 
the Chair of the Judiciary Committee, Senator Durbin, has 
arrived. Mr. Chairman, any remarks you'd like to make?
    Ms. Givens, your opening statement, please.

   STATEMENT OF ALEXANDRA REEVE GIVENS, PRESIDENT AND CHIEF 
    EXECUTIVE OFFICER, CENTER FOR DEMOCRACY AND TECHNOLOGY, 
                         WASHINGTON, DC

    Ms. Givens. Senators, thank you so much for inviting me to 
testify. I'll say that I spent 5 years of my career sitting on 
the benches right behind you, so it's a particular honor to be 
in front of the Judiciary Committee today.
    The world's attention is rightly focused on the 
possibilities and risks of AI systems. As policymakers look to 
address potential harms and promote responsible innovation, 
it's essential that they do so with a focus on human rights, 
including the rights to liberty, privacy, freedom of 
expression, and equal treatment before the law. My testimony is 
going to draw us a little broader to focus on two areas where 
AI systems are already impacting these rights today: the use of 
face recognition by law enforcement and the impact of 
generative AI on elections.
    In previous testimony I've described how AI systems are 
also harming people's civil rights and economic mobility, for 
example, when people are denied a job or housing based on 
inferences made about them by an AI system or are wrongly 
accused of fraud because a government agency uses a flawed AI 
tool. These real-world harms are happening today.
    So I hope the key takeaway of today's hearing is this: That 
at a time when many are discussing the existential risks of AI, 
there are concrete issues on which Congress and the executive 
branch can act right now, and in doing so, demonstrate how AI 
can be governed in a way that centers human rights.
    Today, my organization partnered with the Leadership 
Conference on Civil and Human Rights in over 60 civil society 
groups, urging the Biden administration to expedite its good 
work on these issues. These questions impact all sectors of 
society, and there's much that both Congress and Federal 
agencies can do.
    A few words on AI and government surveillance. Last fall, 
many of us were inspired by images of the brave protests 
happening in Iran. But we weren't the only ones watching those 
protests. In Iran today, face recognition allows the government 
to identify protesters and take action against them. Face 
recognition has also been invoked to police women not correctly 
wearing the hijab, with one official threatening that violators 
would face immediate penalties, such as their bank accounts 
being blocked.
    In this context, AI systems are enabling a repressive 
regime to identify dissenters, surveil them, and automate their 
punishment. AI is used in similar ways in China, as we'll hear, 
and to intimidate peaceful protesters in Uganda, Hong Kong, and 
more. Such examples feel far from the U.S., but there have 
already been abuses here as well. Police in Florida and 
Maryland have used face recognition to identify and harass 
peaceful protesters, chilling Americans' free speech and right 
to peacefully assemble.
    Recently a Georgia resident, Randall Reid, was held in jail 
for 6 days because a face recognition system misidentified him. 
There are other accounts of wrongful arrests, and these are 
likely just the tip of the iceberg. My testimony shows 
recommendations for how Congress could regulate face 
recognition. But importantly, this is just one area where 
Congress could draw a clear contrast to autocratic regimes and 
lead on AI right now.
    Turning to my second example, advances in generative AI are 
spurring creativity and innovation across the country and 
around the world. But they also raise threats for human rights, 
including in the context of elections. In past elections, 
operatives used robocalls and texts to spread deceptive 
information. But now bad actors could easily use AI to 
exponentially grow and personalize voter suppression or other 
targeting.
    Generated images can also twist public understanding of 
political figures and events. Videos and images have already 
been digitally altered to compromise public officials. Fake 
content is now cheaper, easier, and more convincing because of 
the growth of AI tools.
    Now, regulating in this space must be approached with care 
because it involves expressive conduct. There are many 
legitimate reasons why people use software to generate and 
alter content, from artists making new works, to parody, to 
researchers altering celebrity photos to show the hypothetical 
impact of skin cancer. Barring or heavily restricting such 
activities would harm free expression and innovation and 
quickly run afoul of the First Amendment. But this doesn't mean 
that leaders must sit idle. I'll briefly list four ways in 
which Congress could act.
    First, Congress could require the developers of AI systems 
that can be used in high-risk settings to disclose how their 
tools are developed and designed, and require testing for 
elements such as safety, validity, explainability, non-
discrimination, and privacy.
    Second, in some instances the appropriate framework to 
address AI harms will be litigation under existing laws. For 
example, fraud and extortion, harassment, civil rights, 
intellectual property, and product liability. Courts are going 
to have to tackle how these laws apply to new fact patterns, 
and whether and when AI companies bear liability for the 
content their tools produce versus downstream users. But 
Congress can shine a light on these complex issues and act as 
appropriate to fill in gaps through hearings and reports or an 
expert commission.
    Third, there's an urgent need for AI companies to develop 
robust safety standards, as CEOs have said themselves in this 
very building. Governments are pressing companies for near-term 
voluntary agreements. Congress can help ensure that such 
agreements are developed with public visibility and engagement 
from civil society and independent experts.
    Fourth and finally, on deepfake specifically, Congress can 
use its funding and oversight to scale our Nation's capacity at 
this critical time. This should include supporting the 
development of detection technologies and ensuring key 
institutions like law enforcement agencies are equipped to 
quickly debunk manipulated content. My written testimony shares 
more on each of these topics. Thank you, and I look forward to 
your questions.
    [The prepared statement of Ms. Givens appears as a 
submission for the record.]
    Chair Ossoff. Thank you, Ms. Givens. Mr. Cain.

   STATEMENT OF GEOFFREY CAIN, SENIOR FELLOW, FOUNDATION FOR 
             AMERICAN INNOVATION, CHICAGO, ILLINOIS

    Mr. Cain. Chairman Ossoff, Ranking Member Blackburn, and 
Members of the Subcommittee, thank you for the opportunity to 
testify here today. The Chinese Communist Party, or the CCP, 
has seized on artificial intelligence to emerge as the greatest 
threat to democracy and human dignity in the world today.
    As an investigative journalist formerly in China, I was 
among the first people to document and expose the horrific 
surveillance state that oppressed the Uyghur population in the 
far western region of Xinjiang. China used its vast AI-powered 
surveillance system, literally called SkyNet. Since 2017, the 
atrocity has morphed into the largest internment of ethnic 
minorities since the Holocaust. The U.S. State Department calls 
this a genocide.
    In December 2017, I was kicked out of China while 
researching my book, ``The Perfect Police State,'' which is a 
book about the surveillance dystopia that has been built there. 
Ever since then, the AI police state has expanded to alarming 
levels. In 2018, I moved to Turkey and for 3 years tracked down 
defected former intelligence officers from the Ministry of 
State Security, the powerful intelligence body in China with a 
global reach.
    These spies from the Ministry told me that the Uyghur 
genocide was the beginning of an experiment in total AI 
surveillance. The CCP planned to enlist companies and then 
expand the experiment nationwide in China and globally wherever 
possible. In July 2017, China unveiled its National AI 
Development Plan. It called AI a historic opportunity and 
pledged to align development with the government's 
authoritarian values. China has declared its goal as becoming 
the world leader in AI by 2030.
    Recently, the CCP unveiled AI-powered alarms that notify 
the police when someone unfurls a banner, when a foreign 
journalist is traveling to certain parts of the country, and 
when someone from an ethnic minority is present. The companies 
that helped build China's surveillance apparatus operate here 
in America.
    ByteDance, the megafirm that owns TikTok, the popular 
social media app, stands accused by a whistleblower of running 
an in-house CCP committee that had access to all the app's 
data, including data stored in the U.S., contradicting the 
company's past testimony at other Committees. This was all 
according to a court filing.
    Other sanctioned AI firms, such as iFlytek, SenseTime, and 
Megvii have emerged as billion-dollar unicorns with the backing 
of the Chinese state and the involvement of American venture 
capital firms. In April, the Cyberspace Administration of 
China, a very powerful body in the country, announced draft 
regulations for generative AI as well. These draft rules would 
require content produced by chatbots to follow, quote, 
``socialist core values'' and avoid information that 
undermines, quote, ``state unity.''
    Given the CCP's enormous success at censorship so far, I 
believe that it will once again succeed at coercing and 
coopting Chinese and American technology firms. It will 
transform generative AI into a tool of state oppression. We 
must abandon the misguided idealism of working with AI 
companies and government institutes in China.
    As long as the CCP has any control over these technologies, 
AI will not open the democratic discourse, and it will not 
contribute to the betterment of humanity. China cannot be 
trusted to help build the guardrails for AI, which is something 
that Sam Altman, the CEO of OpenAI, recently proposed at a 
Beijing conference on Friday. What should we do?
    First, America should lead the way in building democratic 
human rights first AI standards through United Nations bodies 
and through the International Organization for Standardization, 
or ISO. America must ensure that China's authoritarian agenda 
does not influence global standards.
    Second, we should stop American technologists from helping 
China build its AI surveillance state, which many have been all 
too eager to do. Sanctions and export controls are not enough. 
This Subcommittee may consider drafting a bill that metes out 
prison time for American executives who help develop any form 
of AI in partnership with a Chinese entity that could have 
authoritarian applications.
    Third, we must strengthen our chip supply chains with our 
allies to ensure that China doesn't get access to critical AI 
logic chips. We should treat the CHIPS and Science Act as the 
starting point and not the last step for this goal. We can 
better coordinate with our partners, South Korea, Taiwan, and 
Japan, by upgrading the Chip 4 talks now underway into a formal 
R&D consortium.
    As we enter the unprecedented age of generative AI, we must 
not allow China, a one-party authoritarian state, to infect the 
global ecosystem. We have seen the CCP's willingness to carry 
out genocide against its people with the help of AI 
surveillance systems. Now we must find ways to ensure that the 
words ``never again'' hold true. Thank you, Senators, for 
having me here today. I look forward to answering your 
questions.
    [The prepared statement of Mr. Cain appears as a submission 
for the record.]
    Chair Ossoff. Thank you, Mr. Cain, and thank you to all of 
our panelists for your opening statements. Ms. DeStefano, every 
parent in America can imagine the terror, the bone-chilling 
experience that you had, but you went through it. What was it 
like to hear a voice that you believed was your own 
daughter's--that you believed was your own daughter's 
expressing such distress?
    Ms. DeStefano. It was the most horrified I've ever felt in 
my life, second to actually being bedside to our youngest son, 
who almost passed away from a rare disorder but luckily 
survived. It took me back to that place where you're just 
sitting there helpless. You don't know what to do. You don't 
know what to do next, where to go. The pain, and the fear, and 
the crying, and the sobbing, and the calling out for my help, I 
can't put really into words how haunting that is, and how 
haunting--it will probably last forever just because that's a 
sound you never want to hear.
    Chair Ossoff. And you were told by the authorities after 
reporting this crime that had you wired money or sent money as 
demanded they would investigate, but because no money was sent 
there would be no further investigation. Correct?
    Ms. DeStefano. Correct. Because there----
    Chair Ossoff. And in fact----
    Ms. DeStefano. I'm sorry, go ahead.
    Chair Ossoff. Go ahead, please.
    Ms. DeStefano. Correct. Exactly. Since no crime had been 
committed there was nothing for them to pursue, or then, there 
was no police report that they could take.
    Chair Ossoff. In fact, my staff spoke with the Scottsdale 
PD, and we asked about this and were told the same thing, that 
because you hadn't transferred money, that there wasn't much to 
be done in a criminal context.
    We intend to look into that more deeply at existing wire 
fraud statutes and other State or Federal statutes that may 
create a criminal claim for precisely the circumstances you 
raised. But I think it's clear, Senator Blackburn, Mr. 
Chairman, that this conduct should be criminal and severely 
punished. So you have my commitment to identify paths to 
ensuring that families are protected from what you had to go 
through.
    Dr. Madry, you have specialized--and you're here in your 
personal capacity. I want to emphasize this. But I think it's 
important for the public to understand your credentials, a 
substantial track record of research and leadership at MIT. You 
will soon be joining the team at OpenAI. You're here in your 
personal capacity. Based upon your experience, help the 
Committee to understand other types of emergent scams, con 
jobs, forms of fraud that can emerge similar to what Ms. 
DeStefano experienced.
    Dr. Madry. Thank you. And just for clarification, I 
actually just recently joined OpenAI. So just to clarify this.
    Chair Ossoff. Thank you.
    Dr. Madry. So yes, what Mrs. DeStefano experienced is just 
the beginning, not the end for sure. As I said essentially, 
imagine you have this technology that can impersonate humans as 
long as you don't have to see them, like, in the real life. 
Imagine they are perfect copies. They can really deceive you. 
Actually, they can be better at deceiving you than many humans 
would be because they can pay attention to subtle cues in your 
speech and kind of your cognitive biases.
    So now imagine that someone can just master, you know, 
thousands of copies of such agents, you know. What are the 
possibilities? There is many. I go over many in my written 
statement. You can field persuasion campaigns using that. You 
can imagine that this is how we do advertising in the future. 
It's actually quite scary if you start, you know, to use your 
imagination.
    Chair Ossoff. And in fact, just last week, Dr. Madry, the 
FBI issued a warning that scammers are using AI to create fake 
pornographic videos of victims using images and clips commonly 
found on their social media accounts. And as the full Judiciary 
Committee Chair Durbin and Senator Blackburn works through some 
of the legislation that we are currently moving on, child sex 
abuse material, this is an area that will require our study.
    With my remaining time on this first round, Dr. Madry, I'd 
like to address at the other end of the spectrum, not the daily 
threats to safety, security but what many are discussing as the 
emergence of potential existential threats through lowering the 
cost of access to technologies that enable mass destruction, 
like the development of bioweapons, or catastrophic 
cybersecurity events, or even the emergence of properties of 
these technologies not yet foreseeable that could place the 
species at risk.
    How much credence do you give these warnings? Do you think 
they're overblown? Or do you think we need to be deeply, deeply 
concerned about existential risk?
    Dr. Madry. I think we should be seriously concerned 
because, again, some of these, in particular the ones about 
making it easier to build bioweapons or use them for better, 
you know, breaching security, they are already here. So, like, 
it's a theoretical possibility. So yes. So this is something we 
should be worrying about right now.
    Chair Ossoff. We'll get into that in more detail later. 
Senator Blackburn.
    Senator Blackburn. Thank you, Mr. Chairman. Thank you all 
for your testimony today. Ms. DeStefano, I cannot even imagine 
what you went through during that period of time. But Mr. 
Chairman and Chairman Durbin, I think this points out why we 
need to look at online stalking, online harassment, and putting 
some of the provisions in the online space that we have in the 
physical space, because to be told there's nothing that can be 
done after you experience this--so thank you for your words.
    Mr. Cain, let me come to you. Having followed what has 
happened in China, and I'm so interested in what you learned 
from the former spies who built their surveillance network, I'm 
grateful for the reporting that you've done on this. And what I 
would like to know, and you may want to do this in writing for 
me, which is fine, more details on the types of AI applications 
that are being used to surveil citizens in China. And if it's 
easier to put that in writing and submit it, that'll be fine. I 
think it would be helpful information for us.
    Mr. Cain: Yes, so I would be certainly eager to send you 
something in writing, Senator Blackburn. I could also go over 
some of that here if you have time for it.
    Senator Blackburn. Go ahead. And I'd also like to know 
who--what U.S. companies are sending technology to China that 
they are using for this surveillance.
    Mr. Cain. Certainly. So the Ministry of State Security 
intelligence officers who had recently defected drew diagrams 
for me. I have notebooks full of these diagrams. They show 
where exactly the lines of power are drawn, and what they 
revealed to me when it came to Uyghur populations and minority 
populations in particular, that this was a highly centralized 
system.
    That all cameras, which cover nearly every square inch of 
this region, are scooping up facial recognition data, voice 
recognition data. They've also gathered biometrics on pretty 
much everybody in the region. And this is all scooped up to the 
Ministry of State Security in Beijing. This is the very top of 
the heap. This goes straight up to Xi Jinping himself. This is 
not something that anyone can argue is a local project or is 
being done by local authorities. It is a national plan of 
China's.
    Senator Blackburn. So it's a plan, it's coordinated, it's 
purposeful?
    Mr. Cain. Yes, it's purposeful. And what they told me and 
also they showed me WhatsApp messages--by the way, the Chinese 
Security services use WhatsApp because even they don't trust 
WeChat, the Chinese version, because they think they're getting 
spied on. So they showed me WhatsApp messages with their spy 
handlers in which they're being ordered to create a nationwide 
project and in which the plans are to expand this globally into 
other countries that might want these capabilities.
    Senator Blackburn. Thank you. Ms. Givens, we don't have--
you mentioned privacy in your remarks. And of course, the EU 
has GDPR. We have never been able to get a privacy standard on 
the books.
    And when you look at the development of AI, and you think 
about things that need to be in place before we start down this 
road and look at different applications, whether it's defense 
or logistics or banking or healthcare or entertainment, like a 
lot of my constituents in Tennessee. Logistics, healthcare, 
entertainment, they're doing some good work there. But talk 
about the impact of not having a national consumer privacy 
standard. Talk about the impact that has on AI development.
    Ms. Givens. The need for Federal privacy law in the United 
States is overwhelming because of the real-world harms that are 
happening to people now and because of the way that we're 
seeding global leadership on these issues.
    Just to draw a couple of examples, the way that our current 
privacy regime exists which is a patchwork of State laws, some 
sector-specific laws, relies on notice and choice. This 
abstract idea that users can consent to their data being taken. 
But we know because of the way that AI uses people's data that 
that simply isn't the case. We are beyond a regime where users 
actually opt in to any of these systems about how our 
information is used on a daily basis.
    So we have to have baseline rules of the road established 
in a Federal law to limit the collection, sharing, and use of 
people's private information. When we look at deepfake audio 
and video, the source material for that is people's private 
photos and audio recordings that have been shared and used at 
scale.
    You can also think in the advent of generative AI how much 
information we share with a search bar in any given day. Now, 
think about the private information people are going to be 
sharing with a chatbot. How do we map over to make sure that 
those are secure environments as well and that people can have 
trust in these systems for them to develop?
    The last example I'll give is in the use of AI when it's 
used to discriminate against people in employment, lending, or 
housing. All of that is powered by data driven inferences that 
a privacy law could help address. And the final thing is that 
some of the model privacy laws that have been introduced get at 
these questions of algorithmic transparency and accountability.
    So putting those two things together can be incredibly 
powerful. So we're getting at root cause, the vast amount of 
private information that is so widely available, and then also 
dealing very specifically with these AI use cases as well.
    Senator Blackburn. Thank you. Thank you, Mr. Chairman.
    Chair Ossoff. Thank you, Senator Blackburn. Senator Durbin.
    Chair Durbin. Thank you, Senator Ossoff. It's good to be 
back in the Human Rights Subcommittee. You're doing a great job 
on the Subcommittee. Interesting subject, artificial 
intelligence. I have this hearing today and two different 
briefings this afternoon.
    And it's not unlike that for the last several weeks, two or 
three different briefings a day. And for liberal arts lawyers 
like myself I need them all to try to understand some of the 
technical concepts that we are discussing, and more equally 
important, impact that they're going to have on the lives of 
Americans across the board.
    We have several bills that we've considered before the 
Senate Judiciary Committee which go to the subject of the 
social media platforms, and any responsibility they have. The 
interesting thing is we have five bills, all bipartisan bills, 
Democrats and Republican sponsors, and they all passed this 
Judiciary Committee unanimously. Unanimously. And the premise 
behind them was the notion of responsibility on the part of the 
social media platforms as to what they're posting.
    Under Section 230 for the longest time they didn't pay much 
attention to what was being posted. Now they're starting to pay 
attention. And that's led to a very active discussion within 
the ranks of Democrats and Republicans on the Hill about how 
far we should go in holding them responsible or liable for 
misconduct.
    Ms. Reeve Givens, welcome back to the Senate Judiciary 
Committee. We unanimously approved the STOP CSAM bill, a bill I 
introduced to crack down on proliferation of child sexual abuse 
materials online. Your organization, for some reason, opposed 
the bill. One part of the bill your organization took 
particular issue with is a provision that pierces Section 230 
of the Communications Decency Act and allows CSAM victims to 
sue platforms that host, store, or otherwise make this illegal 
content available.
    We had a classic example at a hearing. A young lady at the 
age of 15 thought she discovered a true boyfriend on the 
internet, was enticed to send sexually explicit videos and 
photographs to this person who put them online. She's tried to 
contact the social media platform that posted them. They 
wouldn't get back to her. They wouldn't accept any 
responsibility. They wouldn't remove them.
    She's been going through this for 20 years now. She's 
attempted suicide three times. She can't hold a job because 
this person eventually, whoever is releasing it, finds her and 
releases the information and the videos again to haunt her 
along the way.
    I heard echoes of your argument against the STOP CSAM Act 
in a recent interview you gave to Bloomberg in discussing 
potential liability of a platform like I've described, when a 
generative AI tool causes harm. You noted that generative AI 
tools, and I quote you now, ``do involve users engaging in 
expressive conduct,'' end of quote. I'm not sure I understand 
the expressive conduct of someone who's posting sexually 
explicit videos of a child. And I also don't understand if it 
would be expressive conduct when I listened to Ms. DeStefano's 
experience.
    It seems as though a company that releases a tool that can 
clone a person's voice should be able to predict some of the 
ways the tool would be misused. And if they don't put 
sufficient safety measures in place, they should be held 
legally accountable. That to me sounds just obvious. So I'm 
worried about your phrase ``expressive conduct'' and your 
opposition to our bill. Would you like to explain?
    Ms. Givens. I would, Senator. I run an organization that 
focuses on human rights and the impact of technology on regular 
people around the world. So the issues that you're raising are 
quite literally the hardest set of opposing tensions that we 
deal with. And the reason we approach these questions the way 
that we do is not by any means that we want to limit the 
ability of victims like that to seek redress.
    It's how we worry about the impact of those legislation 
leading to platforms who have a profit motive and who act when 
they're scared of liability to over-police other types of 
conduct that are lawful and are expressive. So we worry about 
the downstream effects of the heavy thumb of regulation. Now, 
that doesn't mean by any measure that we want companies to turn 
a blind eye to this or to be inactive. We believe in every 
force of market pressure encouraging them to take those 
responsibilities deeply and seriously.
    Chair Durbin. What would you mean by market pressure?
    Ms. Givens. So for example, the way that platforms now have 
advertisers potentially threatening to pull their ads if they 
don't think that they have responsible codes of conduct on 
their platforms, if they're not enforcing that in meaningful 
ways.
    And my organization actively pushes those companies 
ourselves to be responsible and thoughtful in how they're 
acting, to be transparent in what they're doing, to be 
consistent in their approaches. But there's the additional 
hammer of legal liability. We worry about the long-term effects 
of how that changes in platforms and leads to over-takedowns of 
what could be expressive conduct in other settings.
    Chair Durbin. See, you talk about the heavy thumb or 
whatever of government. What we have now is not a heavy thumb. 
We have a hands-off. We stand by the sidelines and watch this 
poor victim, watch what happened to Ms. DeStefano. And to argue 
that we are somehow suppressing the market, you know, perhaps 
we are asking for responsibility, accountability in the market. 
And if you made a decision to put a car on the road that was 
really cheap and you were going to make money on it, 
unfortunately if the brakes are awful, you pay a price for 
that.
    So the expression of the market took second place to the 
safety of people driving the car and those around them. So I 
just have to tell you, I disagree with your premise that the 
market is more important than the individuals who are the 
victims of it.
    And I think that asking people to be held accountable for 
what they have produced and what their actions result in is as 
basic as justice in America. And to ignore that we are to say 
Section 230 or something like it should continue and stop this 
child sexual abuse and material online exploitation, I think it 
goes way too far. Please, respond.
    Ms. Givens. Thank you, Senator. Just to be clear, I'm not 
worried about protecting the market in an abstract notion. I'm 
worried about protecting other users who are posting lawful 
content, but for whom automated content filtering and some of 
the other provisions that companies would use if they were 
worried about legal liability would lead to over-removals.
    So for example, when we apply these types of mandates, if 
companies suddenly get worried--and this has happened in the 
instance of the SESTA/FOSTA bill that was passed by Congress 
with very noble, understandable intentions to address the 
scourge of sex trafficking online. We also now understand 
analyzing those effects that sex workers have had a harder time 
finding online spaces to find community and express their 
concerns.
    And that's been documented in terms of the effects. So 
there is absolutely no questioning the intent of Congress and 
the very real harms that you're trying to address. But I'm 
saying that there are unintended consequences for other lawful 
users in the ecosystem.
    Chair Durbin. So the question is whether we accept the 
premise that those who have these online platforms have any 
responsibility to police content, particularly when we're 
talking about child exploitation and trafficking. For God's 
sake, there's got to be a line we can draw that protects the 
marketplace but still doesn't exploit innocent people. Thank 
you, Mr. Chairman.
    Chair Ossoff. Thank you, Senator Durbin. Senator 
Blumenthal.
    Senator Blumenthal. Thank you. Thanks, Senator Ossoff, for 
having this hearing. As one of the authors of the SESTA/FOSTA 
bill, I happen to be very proud of it. And the consequences 
that you've described for the sex workers have to be addressed. 
But that's not a reason--to try to protect the victims of 
trafficking, or the victims of CSAM, or the victims of 
fentanyl, or the victims of a variety, of a plethora, of other 
evils that the tech platforms know they are enabling and 
propagating and empowering.
    And as somebody who has written a variety of legislation 
and enforced it, legislation can never be wholly good. We have 
to accept that there will be other consequences, intended or 
unintended, that we need to safeguard against. But let me just 
come right to the point here.
    We've had a number of hearings, one of them involving Sam 
Altman. You referred to it, Mr. Cain. In that hearing and in 
the subsequent hearing held in the Committee on Intellectual 
Property and Copyright involving another four or five 
witnesses, everybody agreed Section 230 does not apply to AI. 
Do members of this panel disagree? And if so, please, speak up.
    Dr. Madry. I do not necessarily agree or disagree. 
Actually, I just don't know what is the answer here. Like, it's 
very clear what's happening on the technical level. Now, how do 
we interpret it from the legal perspective? Like, that's 
something that is unclear to me.
    Ms. Givens. So I also don't have a formal position on this. 
I think it's going to be a--this is something courts are going 
to have to figure out, and it's going to be a very fact-
specific inquiry. I think that the arguments for 230 
protections often will not apply in generative AI systems by 
any measure.
    The goal that Section 230 is meant to promote is allowing 
users to create and express themselves in an online 
environment. And often what we're seeing with generative AI is 
less about user expression, right? It's a user putting in a 
query for medical advice, and that's very likely just the 
company spewing something back as opposed to something that the 
user is actually generating or creating.
    So I think there is--and Senator Wyden has been clear about 
this as well. It is a very different fact pattern than what 230 
was generated for. The one exception that I think of is when an 
individual, for example, might use an image generating tool for 
their own expressive purposes. It's them that's using that tool 
in a particular manner. That's where I think there's just a 
little bit of a question of where the facts will go and we need 
to think that through.
    Senator Blumenthal. So how would you enforce Section 230? 
What, by deepfake? By impersonation? I'm not sure I understand.
    Ms. Givens. Oh, no. I'm sorry, Senator. To be clear, there 
are different factual scenarios for how generative AI might be 
used and where the line of liability should fall. There are the 
developers of the AI tool, there are the deployers of the 
generative AI tool, and then there are users. And they're all 
making different choices that might trigger different types of 
liability.
    Senator Blumenthal. But what about the platforms?
    Ms. Givens. So it depends what we're counting as the 
platform in this instant, right? So for example, a generative 
AI tool that would not typically fall in the bucket of 230 by 
any measure. So the point that I'm making is that there are 
moments when it is actually going to be the end user that is 
making that tool do something intentionally bad. And my 
instinct here is that the user should be the one who is mainly 
responsible for what they are doing.
    Senator Blumenthal. Well, my takeaway from this panel is 
that we need to clarify Section 230 to say it doesn't apply to 
AI. Because if it does, we're in a whole new world of hurt.
    Ms. Givens. I do think that there's an awful lot that 
courts will be able to figure out just through this simple 
question of where is aiding and abetting liability. The simple, 
the straightforward shield if you can't litigate it that 230 
provides, I agree that that very unlikely applies to generative 
AI tools at all.
    Instead, I think you are allowed to pierce through and then 
you get into the question of who's doing the conduct where. And 
I think that's where there's going to be a really fruitful 
discussion of where you apportion that liability and the 
responsibility that we want for the platforms for the 
generative AI tools to make sure they can't be misused.
    Senator Blumenthal. Yes, I'm not willing to let the courts 
legislate. I think we have a responsibility to legislate. And 
we have a responsibility to protect people who may be victims, 
and we're moving in that direction. We're also working on 
legislation that would establish an oversight agency, some 
independent entity that would set common-sense rules, and a 
licensing regime for certain uses of AI, not to discourage any 
form of free expression either through that legislation or 
through any rewriting of Section 230.
    We want to encourage innovation and startups in AI the same 
way that Google and Facebook were able to take on the IBMs of 
the world, the great giants, through their innovation. And we 
want to support and encourage people who are doing it in their 
garages, startups.
    But we also want to avoid repeating the mistakes that we've 
done through social media, which literally got halfway around 
the world, as Mark Twain used to say about lies, before the 
Congress got out of bed. And we're still trying to make up for 
lost time there through the Kids Online Safety Act and other 
measures.
    Basic rules of the road can be a sustainable foundation on 
how we move ahead with AI. So I would be interested--my time is 
up here, but any of your written comments on these kinds of 
proposals would be greatly welcome as we go forward. Thanks, 
Mr. Chairman.
    Chair Ossoff. Thank you, Senator Blumenthal. And Ms. 
Givens, I think that this discussion about who has liability is 
essential as we discuss potential regulation and how issues 
that arise from this technology may be treated in the courts. 
Let's discuss that in a civil rights and criminal justice 
context.
    As you noted in your opening statement, there was recently 
a man, Mr. Reid in Atlanta, Georgia, who was arrested and held 
in jail for 6 days on suspicion of a crime committed in a 
different State because of a false match through facial 
recognition technology. So let's just begin by acknowledging 
for the record, Ms. Givens, these tools and technologies are 
hardly foolproof. Correct?
    Ms. Givens. That is absolutely right. And we are seeing the 
errors in those systems deeply impact people's lives today.
    Chair Ossoff. There are a whole range of applications in 
the criminal justice context that raise troubling questions. 
Let's focus on this facial recognition question for the moment, 
and let's discuss a hypothetical.
    If a police department uses an AI-driven facial recognition 
tool and makes an arrest, or perhaps the prosecutor brings a 
charge on the basis of a match using that tool, and it turns 
out that the arrested or charged individual is innocent, and a 
study reveals that the underlying facial recognition tool has 
ingrained in it some racial bias, or is less accurate in 
matching Black faces than white faces, and a civil rights claim 
is brought against the department or against the DA's office, 
where might the liability rest?
    Is it with the department, the prosecutor who used the 
tool? Is it with the producer of the tool? Is it with the AI 
model that the producer of the tool licensed? Is it with 
whoever curated the data that trained the AI model? Your take, 
please.
    Ms. Givens. So sadly, that's not a hypothetical. We've seen 
that these systems do have statistically significant 
differences, particularly for people of darker skin. When we 
look at the few examples that we know in public record of 
misidentifications, those are all Black men so far that have 
been wrongly arrested. And that's only the tip of the iceberg 
because right now people don't know when it's an AI tool, when 
it's face recognition that's being used to just generate their 
arrest.
    So there's a huge information asymmetry here where people 
don't even know that they are the subjects of these tools. And 
that's the case with face recognition. But also, many of the AI 
decisionmaking tools that we could also talk about today, 
whether it's in housing, lending, employment.
    I do think without question, the responsibility first and 
foremost lies with law enforcement in the case of face 
recognition technology. If you are going to be making an 
arrest, you need to make sure that you are doing so under the 
Constitution on a reasonable basis, and you need to be 
complying with all of your constitutional obligations in that 
setting.
    And right now, the accuracy concerns of face recognition 
raise that issue, but also other concerns as well with how the 
use of face recognition impinges on people's ability to express 
themselves, to move freely through society without thinking 
that they are being surveilled.
    So the primary responsibility lies there, and it's not 
going to see action until Congress steps in to legislate. We're 
seeing some States and local governments step in to limit the 
use of face recognition by law enforcement. But we need 
Congress to act to make clear what the obligations are and to 
mandate, for example, that a warrant is required in those 
circumstances.
    Chair Ossoff. Let's take a case that is emerging and will 
likely emerge more frequently when we think about the 
predictive uses of this technology. How vast data sets, much of 
it foraged from public domain, or of course in the case of 
Federal or State or local agencies from law enforcement 
databases or data sets that they may purchase to which they 
license access, being aggregated, analyzed to train models that 
make predictions about risk of criminal activity geographically 
or even at an individual level.
    Let's just take an example where such a model is trained 
based upon public domain and open-source information, or such 
predictions might be made using open-source and publicly 
available information. Is there some point at which that 
becomes itself a form of search by the state?
    Ms. Givens. Those methods raise very deep questions as to 
what could amount to probable cause. The types of examples that 
you're talking about here come up, for instance, where law 
enforcement is doing social media analysis to try and indicate 
who might be culpable of a crime to look at those types of data 
points, or as you mentioned, to do inference analysis.
    And all of them--both raise real questions about the 
accuracy and the likelihood of what they are generating really 
being a legitimate foundation for law enforcement action.
    They also raise--their simple use raise real questions for 
our democracy when we look at the vast amount of data that is 
being collected.
    Again, going back to this question of commercial data 
privacy practices, these are people's Facebook profiles, and 
the images that they've shared of themselves, and what they 
think of as private settings now giving rise to law enforcement 
uses. Law enforcement can purchase data about people from a 
data broker and use that for their investigation, not having to 
go through any of the traditional law enforcement requirements 
for a search.
    So what we are seeing is the proliferation of data creating 
these mechanisms for law enforcement to be able to circumvent 
their legal obligations, and that's something we need to 
fundamentally worry about as well.
    Chair Ossoff. Let's think about it in the context of fair 
housing laws or laws and precedent that establish parameters 
for access to public facilities. Of course, technology is 
emerging and will be used by property owners to screen 
applicants for tenancy embedded within which may be racial 
bias, which on its face would violate fair housing laws. How 
are you seeing these threats to civil liberties and consumer 
rights emerging, and how should Congress be thinking about 
responding?
    Ms. Givens. So sadly, that is also not a hypothetical. 
Those are harms that we're seeing right now. I can give two 
specific instances.
    One is a growing number of landlords who are using face 
recognition technology, ostensibly for security purposes on 
their campuses. But actually, what they are doing is also being 
able to identify somebody who is in arrears on their rent, for 
example, and being able to identify them in that way instead. 
So this is surveillance capabilities for security being instead 
misused in a way that impinges on people's fundamental freedoms 
to go in and out of their home.
    The other area, as you mentioned, is in access to housing. 
We also see this in access to jobs and access to credit and 
lending. Increasingly, we are seeing private sector tools that 
draw together inferences and data points about people.
    For example, their education, history, whether or not 
they've ever had an arrest record against them, what their 
credit score is, whether or not they've ever been in default on 
something, and compiling all of those to see if somebody is 
suitable and eligible as a tenant or as an employee or for a 
particular setting of credit.
    Evidence shows that those often are not good predictors, 
and they're not fair predictors of whether or not somebody 
should be able to have, you know, access to an apartment.
    We know for example that education records, if you look at 
that, and arrest records in our country skew demographically 
against historically marginalized communities. And so when 
we're looking at that versus much more objective data, like, 
``Have you paid your utility bills on time the past couple of 
months,'' we're ingraining metrics and values that can really 
entrench and deepen inequality.
    And right now, there is no oversight of this. There's no 
requirement to be transparent about it that's meaningfully 
enforced, which is why it's important that Federal agencies, 
the Consumer Financial Protection Bureau is doing work on this, 
the Equal Employment Opportunity Commission is doing work on 
this, Congress could also be using its oversight powers to look 
at the existing civil rights protections that we have, see how 
well they're rising to this moment, and then fill in the gaps 
to make sure people are really protected.
    Chair Ossoff. Thank you, Ms. Givens. Dr. Madry, would you 
say that the rate at which this technology is growing in 
capability is linear or exponential? And how do you foresee 
that trend developing over time?
    Dr. Madry. So definitely, if you look at the past 10 years, 
I would say, exponential. In a sense there are things that 10 
years ago seemed like a complete science fiction to me that now 
are just reality. Of course, you know, it's hard to make 
predictions especially about the future, but again, if the last 
10 years tell us anything, we should expect quite a lot of, you 
know, rapid developments ahead of us. But, of course, only time 
will tell.
    Chair Ossoff. To protect against risk, for example, of 
manipulation of biolabs or attacks on nuclear sites and 
critical infrastructure, is your view that emphasis at this 
time should be on guardrails embedded in the AI systems 
themselves, or on defensive technology and innovation in 
cybersecurity?
    Dr. Madry. Well, the answer should be both because 
essentially, like, I think the U.S. Government should really 
get its hands dirty and actually develop AI themselves. And 
that would be on the defensive part. But yes, the guardrails 
are definitely something to think about. We should just keep in 
mind that we can only put the guardrails on things that we 
control, so essentially things that are developed by law-
abiding U.S. or other international companies. But yes, like, 
we should do both.
    Chair Ossoff. Things that we can control and things within 
our jurisdiction. Mr. Cain, you suggested in your opening 
remarks the need for international organizations, whether the 
U.N. or ISO, to be engaged to develop global standards. Talk a 
bit more about your vision for that and how you might see it 
working.
    Mr. Cain. Yes, so thank you, Senator. The ISO has already 
passed a number of global standards and also the UNESCO. So the 
United Nations Science and Education Organization has also done 
its own standards.
    One of the problems with what's been passed so far is that 
they have allowed China to make these moves that sound that--as 
if they are public relations moves.
    So in 2021, there was one standard passed out at UNESCO, 
and later that year the Chinese government said that it was 
going to drop using AI for its social credit systems in China 
to follow these particular standards. But I have sources in 
many of these Chinese firms that develop social credit, and 
they tell me that AI is still being used just wildly without 
any guardrails whatsoever. There's little that that particular 
international standard did.
    So my vision would be something that is more enforceable 
under the law, something that would be required for U.N. member 
states to actually enforce or to create legislation, you know, 
within each member state. So something similar to the 
International Criminal Court or the European Union.
    Now, you know, I do know that this is not something that 
could happen overnight, but with the extent of the technology 
that we're now dealing with I think this might be the only way 
to ensure that bad actors like China or even Russia or others 
can't, you know, trample over the international order.
    Chair Ossoff. Ms. Givens, your perspective please on 
international law and artificial intelligence.
    Ms. Givens. So I think we absolutely need international 
cooperation. Number one, these tools are used across borders. 
They impact people across borders. And number two, I think the 
values that we bring to that conversation, to Mr. Cain's point, 
are deeply important, and the U.S. needs to be in these spaces.
    There are areas where that's happening now, but we should 
think more about that, how that's integrated with the domestic 
agenda. So, for example, the U.S. and the EU Trade and 
Technology Council is an ongoing cooperative effort between the 
U.S. and the European Union to have alignment as they think 
about the governance of AI, and in particular, to develop a 
shared vocabulary around how AI systems work and where 
regulatory interventions can fit in, and to talk about what 
standards for safety and mitigating some of the harms we're 
talking about online look like.
    So I think that's a really important example of how 
cooperation can happen. There's another instance, though, where 
we need to be careful of international agreements actually 
undermining our efforts to regulate these spaces at home. So, 
for example, right now a number of advocacy organizations and 
Members of Congress have spoken out to warn the administration 
that in a trade agreement that has intellectual property 
protections, for example, you don't inadvertently undermine 
domestic efforts to demand transparency of AI systems.
    So I raise that because it's an important example of how 
international and domestic conversations need to sync up with 
one another, and we need to make sure that we are able to 
project our vision of democratic governance and human rights in 
these settings around the world.
    Chair Ossoff. Thank you, Ms. Givens. Senator Blackburn.
    Senator Blackburn. Thank you. Mr. Cain, I wanted to come 
back to you on the second part of my initial question to you 
about what technologies, what U.S. companies may be sending 
technology to China and the CCP that they could use. And do you 
know of any American companies that were involved in creating 
or funding AI tech that was used to surveil citizens in China?
    Mr. Cain. Yes. One of the greatest perpetrators of what you 
are saying is Microsoft. Microsoft has run an AI laboratory in 
Beijing since the late 1990s. It's called Microsoft Research 
Asia. This is the laboratory that went on for two decades to 
train many of the top AI technologists and developers in China, 
many of whom went on to now-sanctioned firms, such as 
SenseTime, Megvii and--I'm sorry, the last firm escapes me at 
the moment, but major, major multibillion-dollar firms.
    Some of these individuals are now sanctioned in addition to 
their companies. And they were directly involved in creating 
the facial recognition and the voice recognition technologies 
that were sold directly to Chinese authorities, to the Defense 
Ministry, to the Public Security Bureau, and to the State 
Security Bureau. Microsoft has created itself at the core of 
the Chinese AI ecosystem.
    And even just--I have an article here in the Financial 
Times just reported just this week. So Microsoft will be moving 
many of the AI developers from this laboratory to Vancouver 
because according to the article, there have been many internal 
discussions about the problematic nature of what has been 
happening over there. That they're getting tangled up in just a 
really bad situation and they need to separate these 
operations.
    Senator Blackburn. Okay. And then you mentioned TikTok and 
ByteDance in your testimony. So touch on how you've witnessed 
the CCP use TikTok and ByteDance to help build out their 
surveillance state.
    Mr. Cain. Yes. ByteDance is--you know, here in America we 
know TikTok as the social media app with the dancing videos and 
the cat videos. In China, ByteDance was directly involved in 
working with the Ministry of Public Security to spread 
propaganda about the Uyghur genocide and about the atrocities 
against human rights there. This was a formal contract. This 
was set up. It was a formal relationship. It did not happen 
under the radar. It's something that ByteDance was directly 
involved in.
    And, you know, personally I find it a bit ludicrous that a 
company that's involved in a genocide overseas can operate so 
openly in America. I think that's a gross, just horrific, you 
know, just a failure to uphold basic principles of rule of law 
and human rights and democracy here. And for that reason, I 
think TikTok should be severely restricted on U.S. soil.
    Senator Blackburn. Okay, thank you. Thank you, Mr. 
Chairman.
    Chair Ossoff. Dr. Madry, in some ways there's a tension 
between what we've thought of traditionally in the AI space 
training models to recognize certain patterns and images and to 
make predictions and on the generative side, the production of 
images, video, audio.
    And there's the potential for the pattern recognition 
capabilities of AI models to be a countermeasure against the 
production of counterfeit, inauthentic content such as what 
terrorized Ms. DeStefano. Which capability is advancing more 
rapidly? The ability to detect what is fake or the ability to 
produce it? And is that something inherent technically or does 
that just reflect where the R&D money is going right now?
    Dr. Madry. That's an excellent question. So in general, 
indeed, there is this kind of complementarity of, you know, 
recognizing if something is fake or not versus being able to 
generate something that can pass as being real or not.
    And in some sense, like, the unfortunate dynamics here is 
that if I have a good detector of a fake content I can turn, 
there is a technical reason for that, it into a even better 
generator of bad content. So what we are essentially, like, 
facing here is this kind of cat-and-mouse game in which kind of 
we really want to be ahead on the right side.
    And this brings me to the other point you mentioned, is the 
funding and incentives. Currently, I do not see that much 
incentives being provided for the detection of the deepfake. 
Like, as far as I know, I'm sure some of the companies are 
doing something, but in the research space definitely more 
activities on generation than on detection. Which makes sense 
because that's what research is about. But I would love if the 
Government provided, in some way, some incentives to much, much 
more work on the detection side.
    Mr. Ossoff. Ms. Givens.
    Ms. Givens. I think that's absolutely right. We need 
extensive and quick research into deepfake detection technology 
and good ways to help authenticate content so that it can be 
trusted in how to make that as effective as possible. I do 
think there are also ways to strongly incentivize the companies 
to play their part in doing this. And a large part of that is 
going to be about how existing law maps onto this.
    We got into a conversation about Section 230, but unlike in 
the 230 context, if a generative AI tool is quite literally 
being used to generate a falsified image, or is allowing 
somebody to create child exploitation material, that's the 
company's own tool that is doing that specific thing and 
surfacing that as a result. And so that's where we may well see 
litigation for defamation or for other things surface onto 
those companies themselves.
    So this is an area, and I talked about this in my 
testimony, where I think Congress can and should pay very close 
attention to whether existing laws are helping address these, 
how the liability is falling, help shape that conversation, and 
use that in addition to some of the market pressure and 
government pressure that's being on the companies right now, to 
step up on some of these questions of how their tools are being 
used and the content that they might generate.
    And I think the combination of those two things, it's not a 
silver bullet but that gets us at least much further than where 
we are now on helping to address these types of concerns.
    Chair Ossoff. Dr. Madry.
    Dr. Madry. I just wanted to add one related piece to that, 
is that in some sense whenever the company that is, like, whose 
tools, like, is providing this AI, is developing this AI, if 
they cooperate they can actually give us a home field advantage 
in this combat because they can provide some watermarking or 
some other capabilities to make it easier to detect that this 
is a fake content.
    Again, this is still all proof-of-concept prototypes right 
now, but it would be great to have incentives as much more work 
in this space. But the point is that we can kind of make it a 
bit easier for us to detect it if we have the cooperation of 
the industry here.
    Chair Ossoff. Ms. Givens, what kinds of First Amendment 
concerns arise?
    Ms. Givens. So as I mentioned in my opening testimony, 
there are very good, lawful, legitimate reasons why people 
might want to manipulate images. Right? There's parody, there's 
my kids messing around to see what images they can create on 
these tools for fun as an experiment. We've seen researchers, 
for example, transform photos of American cities to show what 
they would have looked like had they been subject to the 
extensive bombing that happened in Syria as a way of public 
education.
    These are all good reasons why generative images and 
manipulated images might have useful purposes and should be 
treated as a form of expressive conduct. So the tricky question 
comes in on how we incentivize the companies to address harmful 
misuses of that technology and put in the safety guards that 
they can to address that.
    For example, there are some companies already that say 
images of political figures running for public office simply 
cannot be manipulated on their platforms. The technology 
doesn't allow it so that they do not contribute to election 
related deepfakes.
    There are things that companies can do, but how we create 
that balance between what the companies are choosing for their 
content policies in a way that promotes safety but also allows 
parity in the expressive activities that our Constitution 
protects and that as a society we will want to foster, that is 
the challenge before us right now on how we balance those two 
issues.
    Chair Ossoff. Dr. Madry which emergent capabilities or 
capabilities that are here today most excite you?
    Dr. Madry. Excite me? That's an interesting choice of the 
word.
    Chair Ossoff. Or if you're not excitable, which do you 
believe have the greatest potential to support and promote 
human flourishing, human health, human well-being, and human 
freedom?
    Dr. Madry. Okay, so that's different because we were 
talking about all the bad users. So I'm not excited about any 
of them, but I'm definitely very excited about many of the 
potential outcomes. To me, the biggest vision that I have of 
positive vision about AI, and hopefully it's relatively close, 
is essentially having this personal tutor, personal kind of, 
like, essentially tutor who understands us, understands our 
learning deficiencies if we have them, understanding how we 
learn, and helping us learn about different issues.
    So essentially you can use generative AI to kind of help 
you kind of look at the solutions to your problems and seeing, 
you know, what mistakes you are making, explaining these 
mistakes and so on. So we are seeing some early work on this. 
In particular, Khan Academy is working on such technology and 
I'm extremely excited about the impact it would have on the 
humanity if this kind of really high-quality education could be 
available to everyone at minimal and ideally no cost.
    Chair Ossoff. And Ms. Givens, we'll give everyone the 
opportunity to say what they're most potentially enthusiastic 
about. But I just want to--because this question on education 
it raises, I'm afraid to say, Dr. Madry, a question about risk.
    You know, when we think about the way that we sort 
children, the way that standardized testing regimes function to 
sort young people toward careers, toward educational 
opportunities, the capacity to make judgments about human 
potential on the basis of data that to this point was not 
intelligible is vast.
    The potential to use it for good, to provide personalized 
educational experiences that meet special needs is vast. But 
so, too, is the potential for this to constrain human freedom 
and to determine the choices and futures available to a human 
being from a very young age. Ms. Givens, how should we be 
thinking about regulation or best practices or standards in 
education?
    Ms. Givens. The way you phrase it is so beautifully put. 
This is a privacy issue. My goodness. The type of interaction 
that we have with those systems, all of that potential--and 
there is so much--if that is also used to profile you, to say 
what learning differences you have as you're going through an 
experience, if we don't have strong Federal privacy regulation, 
anybody could get their hand on that data and the company could 
just bury it somewhere deep in their terms of service, and you 
wouldn't even know when you start using that tool.
    So this is why we need rules of the road. We need rules of 
the road for privacy. We need rules of the road for how people 
can use this information and for people to be able to sue and 
bring a cause of action if they are being discriminated against 
based on this type of information, for example.
    And then, of course, you mentioned the need for responsible 
design. So there's legal liability but even absent individuals 
vindicating their rights. We also need to make sure that 
companies are coming into this with a mindset of safety.
    And that's where entities in particular working in the 
education space have to be committed to equity and serving the 
person first, making sure that what they're doing is 
accommodating people's needs in learning, but not triaging the 
top students from the bottom and leaving the bottom just to 
keep circulating in that ever-reinforcing pattern.
    That's where questions--it's going to be hard for Congress 
to very specifically mandate exactly how those tools should 
work. But that's where general-purpose legislation like 
algorithmic accountability, mandating transparency, mandating 
risk assessments for what types of harms might result from an 
algorithmic system, and having companies have to disclose how 
they're addressing those harms, that's how policymakers and 
regulators would be able to understand the risks of those tools 
and take action against them when they're harming people.
    Chair Ossoff. Ms. DeStefano, in many ways your family's 
story sets the tone for this hearing. And you have opened many 
eyes across the Nation to the kind of horrifying risk that 
Americans face from the abuse and misuse of this technology. 
And I'm grateful to you for coming and sharing your story with 
us. Before we close the hearing, are there any final 
reflections or comments that you'd like to make?
    Ms. DeStefano. What I experienced was horrible. It was one 
of the worst 4 minutes of my life. That being said, that 
doesn't mean that all AI is obviously evil. Listening to a lot 
of different areas that it can be used for good is really 
inspiring. We have a young son with a genetic disorder, and my 
daughter, Aubrey, we also spoke about, went through speech 
therapy for 6 years.
    The advancements and accessibility that AI can help these 
children grow and overcome disabilities is incredible. It was 
very difficult for us. That's why I knew what an unknown number 
would often mean, a doctor's office or hospital, through 
personal experience. It was very difficult to be able to get 
her or both of them into developmental pediatrics and speech 
pathology, etc., to help them improve and overcome their 
disabilities.
    So I think AI, by allowing education or accessibility to 
certain types of specialized medicine and specialized care, 
that can be really beneficial. So I don't want to speak 
horribly negative about AI. What happened to me with my 
daughter was the tragic side of AI. But in the other sense, 
too, there's a lot of hopeful advancements that AI will do to 
improve life as well, so.
    Chair Ossoff. Thank you. Ms. DeStefano. And Dr. Madry, both 
Mr. Cain and Ms. Givens weighed in on international law, 
international agreements, potential for the need for there to 
be an international regulatory agency. Your view on that as a 
scientist, engineer, and technologist, what is it that would 
require inspection? What are the standards, or thresholds, or 
capabilities that such an entity would regulate?
    Dr. Madry. Well, essentially, usually--first of all, I 
think we will only be learning what it is that we should be 
looking for. So that's where you want to have this structure 
and agency in place that has close touch and is paying 
attention to how things develop. If you ask me about the 
capability threshold, I would put it essentially roughly at the 
state-of-the-art right now. And then as we see how technology 
develops which again we could be able to keep close track of, 
and what are the new risks, we might either lower it or make it 
higher.
    But yes, I would just want to understand exactly how is 
this AI used, for which purposes, to what extent can we 
mitigate certain bad users of this, and essentially also 
understand where we as the whole world, not only the U.S., are 
in terms of, you know, emerging AI capabilities. So if there is 
some threshold to be exceeded, well, we want to know it sooner 
than later.
    Chair Ossoff. Ms. Givens, what actions must Congress take 
to stay on the critical path toward ensuring that the emergence 
of this technology facilitates human flourishing and human 
freedom rather than enabling the abuse of power?
    Ms. Givens. Congress needs to look at specific use cases, 
like the face recognition example that I gave, which probably 
requires specific legislation to address those harms. But then 
there's an across-the-board effort that Congress could make as 
well, which is to get to this question of mandating 
transparency and mandating disclosures of how companies are 
looking at questions of safety, validity, their fitness for 
purpose, whether they discriminate, whether they violate 
people's privacy.
    We need to establish that as the baseline analysis for any 
company whose tool could have a high-risk use to go through 
that process, and to do it not just internally but to publicly 
disclose how they're thinking about those risks and what they 
are doing to mitigate those risks. We can't have accountability 
without that baseline rule of the road because we literally 
don't know how the harms are going to manifest, and we can't 
just have individuals trying to fight this David versus Goliath 
battle.
    So if we talk about algorithmic accountability, Congress 
can step in there in a meaningful way to try and really start 
that conversation, and then have ongoing oversight of how well 
our civil rights laws and product liability laws are rising to 
the occasion as well. So I think there's steps Congress can 
take now, like legislating around algorithmic accountability, 
and then there's oversight power that Congress can have, too, 
of how the sector continues to evolve.
    And above all, I think one of the big pieces--somebody 
mentioned earlier that they're not a technologist. Senator 
Durbin said that. We need non-technologists to feel they have a 
seat at the table. We need public voices to have a seat in 
these conversations. So right now, governments around the world 
are talking to some of the largest companies about the safety 
standards they're going to adopt, and that's good.
    But there's a role for Congress to help make that a much 
more public conversation, where civil society advocates and 
regular people have a seat at the table as well. And that's 
another area where Congress can use its oversight authorities 
now to help drive that conversation forward quickly but in a 
meaningful way.
    Chair Ossoff. I want to thank all of our witnesses for 
appearing today and for helping us work through these 
questions. I thank my colleagues who attended for a productive 
discussion.
    After what we've heard today about the risks and the 
opportunities, it is clear that the Senate must continue and 
accelerate our study of machine learning, of artificial 
intelligence, and Ms. Givens, to the point you made and Senator 
Blackburn made, get our act together on a national privacy law. 
Without national privacy legislation, our efforts to control 
the abuse of these technologies are substantially reduced. And 
so that is an urgent task for the U.S. Congress.
    The hearing record will remain open for 1 week for 
statements to be submitted into the record. Questions for the 
record may be submitted by Senators by 5 p.m. on Tuesday, June 
21st. The hearing is adjourned.
    [Whereupon, at 4:11 p.m., the hearing was adjourned.]
    [Additional material submitted for the record follows.]

                            A P P E N D I X

              Additional Material Submitted for the Record

[GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]


                                 <all>
</pre></body></html>
